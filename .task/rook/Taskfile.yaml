---
version: "3"

vars:
  TIME: '{{now | date "150405"}}'

includes:
  k8s:
    internal: true
    taskfile: ../k8s
  talos:
    internal: true
    taskfile: ../talos

tasks:
  toolbox:
    aliases: [tb]
    desc: Launches a shell of a Rook Ceph Toolbox pod deployment, then cleans up.
    cmds:
      - kubectl delete -n rook-ceph jobs/rook-ceph-toolbox-job || true
      - curl -sL https://raw.githubusercontent.com/rook/rook/v1.11.9/deploy/examples/toolbox-job.yaml | yq '.spec.template.spec.containers[0].command = ["/bin/bash", "-c"] | .spec.template.spec.containers[0].args = ["sleep 2147483647"]' | kubectl apply -f -
      - defer: curl -sL https://raw.githubusercontent.com/rook/rook/v1.11.9/deploy/examples/toolbox-job.yaml | kubectl delete -f -
      - task: k8s:wait-pod-running
        vars: { namespace: rook-ceph, name: -l job-name=rook-ceph-toolbox-job }
      - kubectl exec -n rook-ceph  jobs/rook-ceph-toolbox-job -it -- /bin/bash

  osd-prepare-logs:
    aliases: [osdlogs]
    desc: Stream all logs for the `osd-prepare` Job.
    cmds:
      - while true; do kubectl logs -n rook-ceph -l app=rook-ceph-osd-prepare -c provision --tail 999999999999999999 -f 2>&1 | grep -v "No resources found" | tee --append /tmp/rook-ceph-osd-prepare-{{- .TIME -}}.log; done

  zap-disk:
    desc: Prepare a disk to be used as a Ceph OSD on specified node by zapping all data and partition data
    dir: /{{.ROOT_DIR}}/.task/rook
    requires:
      vars: [node,disk]
    vars:
      JOB_NAME: zap-disk-{{- .node -}}-{{- .TIME -}}
      NODE: '{{.node}}'
      CEPH_DISK: '{{.disk}}'
    env: &task-vars
      NODE: "{{.NODE}}"
      CEPH_DISK: "{{.CEPH_DISK}}"
      TIME: "{{.TIME}}"
      JOB_NAME: "{{.JOB_NAME}}"
    cmds:
      - envsubst < <(cat zap-disk-job.tmpl.yaml) | kubectl apply -f -
      - |-
        kubectl -n kube-system logs job/{{.JOB_NAME}} -f || true;
        until kubectl -n kube-system wait job/{{.JOB_NAME}} --for condition=complete --timeout=2s; do
          echo "Job {{.JOB_NAME}} is still running, logs:" &&
          kubectl -n kube-system logs job/{{.JOB_NAME}} -f || true;
        done;
      - defer: kubectl -n kube-system delete job {{.JOB_NAME}}
    preconditions:
      - sh: test -f zap-disk-job.tmpl.yaml

  wipe-state:
    desc: Wipe all Ceph state on specified node.
    dir: /{{.ROOT_DIR}}/.task/rook
    requires:
      vars: [node]
    vars:
      JOB_NAME: zap-disk-{{- .node -}}-{{- .TIME -}}
      NODE: '{{.node}}'
    env: *task-vars
    cmds:
      - envsubst < <(cat wipe-rook-state-job.tmpl.yaml) | kubectl apply -f -
      - |-
        kubectl -n kube-system logs job/{{.JOB_NAME}} -f || true;
        until kubectl -n kube-system wait job/{{.JOB_NAME}} --for condition=complete --timeout=2s; do
          echo "Job {{.JOB_NAME}} is still running, logs:" &&
          kubectl -n kube-system logs job/{{.JOB_NAME}} -f || true;
        done;
      - defer: kubectl -n kube-system delete job {{.JOB_NAME}}
    preconditions:
      - sh: test -f wipe-rook-state-job.tmpl.yaml

  wipe-node:
    desc: Trigger a wipe of all Rook-Ceph data on specified node.
    dir: /{{.ROOT_DIR}}/.task/rook
    requires:
      vars: [node,disk]
    cmds:
      - task: zap-disk
        vars: { node: '{{.node}}', disk: '{{.disk}}' }
      - task: wipe-state
        vars: { node: '{{.node}}' }
      - task: talos:reboot
        vars: { node: '{{.node}}' }

  wipe-nodes-main:
    desc: Wipe all nodes in cluster 'main'
    dir: /{{.ROOT_DIR}}/.task/rook
    cmds:
      - task: wipe-node
        vars:
          node: augustus
          disk: /dev/disk/by-id/nvme-Lexar_SSD_NM7A1_1TB_NGF606R002706P2200
      - task: wipe-node
        vars:
          node: nero
          disk: /dev/disk/by-id/nvme-Lexar_SSD_NM7A1_1TB_NGF606R001841P2200
      - task: wipe-node
        vars:
          node: titus
          disk: /dev/disk/by-id/nvme-Lexar_SSD_NM7A1_1TB_NGF606R002283P2200

  force-delete-cluster:
    desc: |-
      Sometimes Rook seems to fail applying the cluster and want to delete it before it even gets anything set up, this Task will force delete all finalizers to delete all unready Ceph resources.
    dir: /{{.ROOT_DIR}}/.task/rook
    requires:
      vars: [cluster]
    cmds:
      - task: k8s:cluster-switch
        vars: { cluster: '{{.cluster}}'}
      - kubectl --namespace rook-ceph patch cephcluster rook-ceph --type merge -p '{"spec":{"cleanupPolicy":{"confirmation":"yes-really-destroy-data"}}}' || true
      - helm uninstall -n rook-ceph rook-ceph-cluster && true || true
      - |-
        for CRD in $(kubectl get crd -n rook-ceph | awk '/ceph.rook.io/ {print $1}'); do
            kubectl get -n rook-ceph "$CRD" -o name | \
            xargs -I {} kubectl patch -n rook-ceph {} --type merge -p '{"metadata":{"finalizers": []}}' && true || true
        done
      - |-
        kubectl -n rook-ceph patch configmap rook-ceph-mon-endpoints --type merge -p '{"metadata":{"finalizers": []}}' && true || true
        kubectl -n rook-ceph patch secrets rook-ceph-mon --type merge -p '{"metadata":{"finalizers": []}}' && true || true

  reinstall:
    desc: |-
      Assuming Flux and resource names, suspends master ks.yaml (Flux Kustomization), suspends ks.yaml for Rook-Ceph and cluster,
      suspends HelmReleases for Rook-Ceph and cluster, deletes cluster HelmRelease, patches Ceph CR and cm/secret finalizers, removes Rook-Ceph HR and namespace.
      Then, reconcile master, Rook-Ceph and cluster ks.yaml.
      Attempts to delete assuming the following works: https://github.com/rook/rook/blob/master/design/ceph/ceph-cluster-cleanup.md
    prompt: This will completely reinstall rook-ceph and wipe all data on the drives... Continue?
    dir: /{{.ROOT_DIR}}/.task/rook
    requires:
      vars: [cluster]
    vars:
      nodes:
        sh: kubectl --context admin@{{.cluster}} get nodes -o name | awk -F "/" '{print $2}'
    cmds:
      - task: k8s:cluster-switch
        vars: { cluster: '{{.cluster}}' }

        # stop Flux from maintaining rook-ceph
      - flux suspend ks flux
      - flux suspend ks rook-ceph
      - flux suspend ks rook-ceph-cluster

      # Wipe rook-ceph and all disks
      - task: force-delete-cluster
        vars: { cluster: '{{.cluster}}' }
      - helm uninstall -n rook-ceph rook-ceph && true || true
      - kubectl get namespaces rook-ceph && until kubectl delete namespaces rook-ceph; do kubectl get namespaces rook-ceph -o jsonpath="{.status}"; done || true
      # - for: { var: nodes }
      #   task: wipe-state
      #   vars: { node: '{{.ITEM}}' }
      - task: wipe-nodes-{{.cluster}}


      # Install (add back namespace)
      - kubectl apply -f /{{.ROOT_DIR}}/kubernetes/{{.cluster}}/apps/rook-ceph/namespace.yaml
      - flux suspend ks flux && flux resume ks flux
      - flux suspend ks rook-ceph && flux resume ks rook-ceph
      - flux suspend ks rook-ceph-cluster && flux resume ks rook-ceph-cluster