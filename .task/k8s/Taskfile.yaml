---
version: "3"

tasks:
  cluster-switch:
    run: once
    aliases: [ctx]
    desc: Switch kubeconfig current-context to another cluster
    requires:
      vars: [cluster]
    cmd: "sed -i 's/current-context: admin@.*/current-context: admin@{{.cluster}}/g' ~/.kube/config || true"

  wait-pod-running:
    aliases: [waitr]
    internal: true
    desc: Wait for a job's pod to change its status to running
    requires:
      vars: [namespace, name]
    cmds:
      - until [[ $(kubectl -n {{.namespace}} get pod {{.name}} -o jsonpath='{.items[*].status.phase}') == "Running" ]]; do sleep 1; done

  wait-finish:
    internal: true
    desc: Wait for a job to finish
    requires:
      vars: [ns, type, name]
    cmd: |-
      until kubectl -n {{.ns}} wait {{.type}}/{{.name}} --for condition=complete --timeout=2s; do
        echo "{{.name}} is still running, logs:" && kubectl -n {{.ns}} logs {{.type}}/{{.name}} --since 2s -f || true;
      done

  flux-bootstrap-apply:
    desc: Bootstrap Flux onto a new cluster
    requires:
      vars: [cluster]
    cmds:
      - kubectl apply --server-side --kustomize /{{.ROOT_DIR}}/kubernetes/{{.cluster}}/bootstrap/flux
      - sops --decrypt /{{.ROOT_DIR}}/kubernetes/{{.cluster}}/bootstrap/flux/age-key.sops.yaml | kubectl apply -f -
      - kubectl apply --server-side --kustomize /{{.ROOT_DIR}}/kubernetes/{{.cluster}}/flux/vars
      - kubectl apply --server-side --kustomize /{{.ROOT_DIR}}/kubernetes/{{.cluster}}/flux/config
    preconditions:
      - msg: Private age key not found
        sh: test -f /{{.ROOT_DIR}}/age.key
      - msg: Flux bootstrap kustomization.yaml is required
        sh: test -f /{{.ROOT_DIR}}/kubernetes/{{.cluster}}/bootstrap/flux/kustomization.yaml
      - msg: Flux age-key not found
        sh: test -f /{{.ROOT_DIR}}/kubernetes/{{.cluster}}/bootstrap/flux/age-key.sops.yaml

  cilium-bootstrap-apply:
    desc: Bootstrap Cilium onto a new cluster
    requires:
      vars: [cluster]
    cmds:
      - kubectl delete configmap -n kube-system cilium-config || true
      - kubectl delete daemonset -n kube-system cilium || true
      - kubectl delete deployment -n kube-system cilium-operator hubble-ui hubble-relay || true
      - cmd: kustomize build --enable-helm /{{.ROOT_DIR}}/kubernetes/{{.cluster}}/bootstrap/cilium/ | envsubst | kubectl delete -f -
        ignore_error: true
      - kustomize build --enable-helm /{{.ROOT_DIR}}/kubernetes/{{.cluster}}/bootstrap/cilium/ | envsubst | kubectl apply -f -
    preconditions:
      - msg: cilium boostrap files not found
        sh: test -f /{{.ROOT_DIR}}/kubernetes/{{.cluster}}/bootstrap/cilium/kustomization.yaml

  csr-bootstrap-apply:
    desc: Bootstrap kubelet-csr-approver
    requires:
      vars: [cluster]
    cmds:
      - cmd: kustomize build --enable-helm /{{.ROOT_DIR}}/kubernetes/{{.cluster}}/bootstrap/kubelet-csr-approver/ | envsubst | kubectl delete -f -
        ignore_error: true
      - kustomize build --enable-helm /{{.ROOT_DIR}}/kubernetes/{{.cluster}}/bootstrap/kubelet-csr-approver/ | envsubst | kubectl apply -f -
    preconditions:
      - msg: kubelet-csr-approver bootstrap files not found
        sh: test -f /{{.ROOT_DIR}}/kubernetes/{{.cluster}}/bootstrap/kubelet-csr-approver/kustomization.yaml

  mount:
    desc: Mount a PersistentVolumeClaim to a pod temporarily
    summary: |
      Args:
        cluster: Cluster to run command against (required)
        ns: Namespace the PVC is in (default: default)
        claim: PVC to mount (required)
    interactive: true
    vars:
      cluster: "{{ .cluster }}"
      ns: '{{ .ns }}'
      claim: "{{ .claim }}"
    requires:
      vars:
        - claim
        - ns
        - cluster
    cmds:
      - task: cluster-switch
        vars: { cluster: "{{ .cluster }}" }
      - |
        kubectl run -n {{.ns}} debug-{{.claim}} -i --tty --rm --image=null --privileged --overrides='
          {
            "apiVersion": "v1",
            "spec": {
              "containers": [
                {
                  "name": "debug",
                  "image": "ghcr.io/onedr0p/alpine:rolling",
                  "command": [
                    "/bin/bash"
                  ],
                  "stdin": true,
                  "stdinOnce": true,
                  "tty": true,
                  "volumeMounts": [
                    {
                      "name": "claim",
                      "mountPath": "/mnt/claim"
                    }
                  ]
                }
              ],
              "volumes": [
                {
                  "name": "claim",
                  "persistentVolumeClaim": {
                    "claimName": "{{.claim}}"
                  }
                }
              ],
              "restartPolicy": "Never"
            }
          }'
    preconditions:
      - sh: kubectl --context admin@{{.cluster}} -n {{.ns}} get pvc {{.claim}}
        msg: "PVC not found"